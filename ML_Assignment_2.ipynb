{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b38cf-0e35-4ec4-868c-406b0257de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Machine Learning - Assignment 2\n",
    "\n",
    "## Data Science Masters\n",
    "\n",
    "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "**Overfitting:** Overfitting occurs when a model learns the training data too well, including its noise and outliers, which negatively impacts the model's performance on new, unseen data. The model becomes too complex and captures patterns that do not generalize to new data.\n",
    "\n",
    "**Consequences of Overfitting:**\n",
    "- Poor generalization to new data\n",
    "- High variance in model performance\n",
    "- Increased risk of capturing noise as meaningful patterns\n",
    "\n",
    "**Mitigation Techniques:**\n",
    "- Use more training data\n",
    "- Implement cross-validation\n",
    "- Apply regularization techniques like L1 or L2 regularization\n",
    "- Prune decision trees\n",
    "- Use simpler models\n",
    "\n",
    "**Underfitting:** Underfitting occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and test data. The model fails to learn the relationships within the data.\n",
    "\n",
    "**Consequences of Underfitting:**\n",
    "- Poor model performance on training and test data\n",
    "- High bias in model predictions\n",
    "- Failure to capture important patterns\n",
    "\n",
    "**Mitigation Techniques:**\n",
    "- Increase model complexity\n",
    "- Use more relevant features\n",
    "- Reduce regularization\n",
    "- Train the model for a longer duration\n",
    "\n",
    "### Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "To reduce overfitting, consider the following techniques:\n",
    "- **Cross-validation:** Use cross-validation techniques to ensure the model generalizes well to unseen data.\n",
    "- **Regularization:** Apply L1 (Lasso) or L2 (Ridge) regularization to penalize complex models.\n",
    "- **Pruning:** Prune decision trees to remove unnecessary branches.\n",
    "- **Early Stopping:** Stop training when the model's performance on a validation set starts to degrade.\n",
    "- **Ensemble Methods:** Use techniques like bagging and boosting to combine multiple models and reduce overfitting.\n",
    "- **Increase Training Data:** Gather more data to train the model, helping it generalize better.\n",
    "\n",
    "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "**Underfitting:** Underfitting happens when a model is too simplistic to capture the underlying patterns in the data, resulting in poor performance on both training and test datasets.\n",
    "\n",
    "**Scenarios Where Underfitting Can Occur:**\n",
    "- Using a linear model to fit non-linear data\n",
    "- Insufficient model training\n",
    "- High regularization that constrains the model too much\n",
    "- Inadequate features or too few features in the model\n",
    "- Extremely noisy data where the model fails to capture the true signal\n",
    "\n",
    "### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "- **Bias:** Error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to miss relevant relations (underfitting).\n",
    "- **Variance:** Error due to too much complexity in the learning algorithm. High variance can cause the model to model the random noise in the training data (overfitting).\n",
    "\n",
    "The tradeoff between bias and variance affects model performance:\n",
    "- **High Bias:** Leads to systematic errors and underfitting, where the model cannot capture the underlying trend.\n",
    "- **High Variance:** Leads to overfitting, where the model captures noise and fluctuations in the training data.\n",
    "\n",
    "### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "**Detecting Overfitting:**\n",
    "- Large difference between training and test/validation performance\n",
    "- High accuracy on training data but low accuracy on test data\n",
    "- Use cross-validation to check consistency across different data splits\n",
    "\n",
    "**Detecting Underfitting:**\n",
    "- Similar poor performance on both training and test/validation data\n",
    "- Model fails to capture patterns in the data\n",
    "- High bias errors\n",
    "\n",
    "**Determining Model Status:**\n",
    "- **Overfitting:** High training accuracy but low test/validation accuracy\n",
    "- **Underfitting:** Low accuracy on both training and test/validation sets\n",
    "\n",
    "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "**Bias:**\n",
    "- Error due to incorrect assumptions in the model\n",
    "- High bias models: Linear regression with limited features\n",
    "- Performance: High error on both training and test data\n",
    "\n",
    "**Variance:**\n",
    "- Error due to sensitivity to small fluctuations in the training data\n",
    "- High variance models: Decision trees, k-nearest neighbors with small k\n",
    "- Performance: Low training error but high test error\n",
    "\n",
    "### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "**Regularization:** Regularization is a technique used to prevent overfitting by adding a penalty to the model complexity. It discourages the model from learning noise in the training data.\n",
    "\n",
    "**Common Regularization Techniques:**\n",
    "- **L1 Regularization (Lasso):** Adds the absolute value of coefficients as a penalty term to the loss function, leading to sparse models.\n",
    "- **L2 Regularization (Ridge):** Adds the squared value of coefficients as a penalty term to the loss function, leading to smaller coefficient values.\n",
    "- **Dropout:** Randomly drops neurons during training to prevent over-reliance on certain paths.\n",
    "- **Elastic Net:** Combines L1 and L2 regularization to take advantage of both techniques.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
